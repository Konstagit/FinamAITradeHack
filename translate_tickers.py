import json, re, ast, requests
from typing import Dict, Optional, List
from pathlib import Path

MOEX_BASE = "https://iss.moex.com/iss"

def normalize_name(s: str) -> str:
    s = s.lower().strip()
    s = s.replace("—ë", "–µ")
    # –ª–∞—Ç/–∫–∏—Ä –º–∏–∫—Å—ã –¥–ª—è "pao/pjsc/oao" –∏ —Ç.–ø.
    org_trash = [
        r"\b(pao|pjsc|oao|zao|ao)\b",
        r"\b(–ø–∞–æ|–æ–∞–æ|–∑–∞–æ|–∞–æ|–æ–æ–æ)\b",
        r"\b(–øao)\b",
    ]
    for pat in org_trash:
        s = re.sub(pat, " ", s)
    # —É–±—Ä–∞—Ç—å —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    s = re.sub(r"[\.\,\(\)\[\]\"\'/\\]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def is_pref_from_shortname(shortname: str) -> bool:
    sn = shortname.lower()
    return any(x in sn for x in [" –∞–ø", " –ø—Ä–µ—Ñ", " pref", " pr"])

class MOEXTickerMapper:
    def __init__(self, manual_mappings: Dict[str, str] = None, include_tqtf: bool = True,
                 log_not_found: bool = True, drop_records_without_tickers: bool = True):
        self.cache: Dict[str, str] = {}
        self.manual_mappings = manual_mappings or {}
        self.log_not_found = log_not_found                   # –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å ¬´–Ω–µ –Ω–∞–π–¥–µ–Ω–æ¬ª
        self.drop_records_without_tickers = drop_records_without_tickers  # —É–¥–∞–ª—è—Ç—å –∑–∞–ø–∏—Å–∏ –±–µ–∑ —Ç–∏–∫–µ—Ä–æ–≤
        self._build_offline_directory(include_tqtf=include_tqtf)
        self._apply_manual_mappings()

    def _fetch_board(self, board: str):
        url = (
            f"{MOEX_BASE}/engines/stock/markets/shares/boards/{board}/securities.json"
            "?iss.meta=off&iss.only=securities&securities.columns=SECID,SHORTNAME,NAME,SECNAME,LATNAME,ISIN"
        )
        r = requests.get(url, timeout=20)
        r.raise_for_status()
        blk = r.json().get("securities", {})
        columns = blk.get("columns", []) or []
        data = blk.get("data", []) or []
        # –≤–µ—Ä–Ω—ë–º –∏ –∫–æ–ª–æ–Ω–∫–∏, –∏ —Å—Ç—Ä–æ–∫–∏, —á—Ç–æ–±—ã –¥–∞–ª—å—à–µ –º–∞–ø–ø–∏—Ç—å –ø–æ –∏–º–µ–Ω–∏
        return columns, data

    def _build_offline_directory(self, include_tqtf: bool):
        print("–ì—Ä—É–∂—É —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ TQBR‚Ä¶")
        cols, data = self._fetch_board("TQBR")
        if include_tqtf:
            print("–ì—Ä—É–∂—É —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ TQTF‚Ä¶")
            cols2, data2 = self._fetch_board("TQTF")
            # –µ—Å–ª–∏ –Ω–∞–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è, –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–≤–µ–¥—ë–º —Å—Ç—Ä–æ–∫–∏ –∫–æ –≤—Ç–æ—Ä–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É, –¥–æ—Å—Ç–∞–≤–∞—è –ø–æ –∏–º–µ–Ω–∏
            # –Ω–æ –ø—Ä–æ—â–µ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –∫–∞–∫ –µ—Å—Ç—å ‚Äî –±—É–¥–µ–º —á–∏—Ç–∞—Ç—å –ø–æ –∏–º–µ–Ω–∞–º –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞
            data += data2
            # columns –Ω–∞–º –≤–∞–∂–Ω—ã —Ç–æ–ª—å–∫–æ –∫–∞–∫ ¬´—Å–ª–æ–≤–∞—Ä—å –∏–º—ë–Ω¬ª ‚Äî –≤–æ–∑—å–º—ë–º –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞,
            # –∞ –≤ —Ü–∏–∫–ª–µ –±—É–¥–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã —á–µ—Ä–µ–∑ cols.index(...) —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞–ª–∏—á–∏—è

        # –±–µ–∑–æ–ø–∞—Å–Ω–æ –±–µ—Ä—ë–º –∏–Ω–¥–µ–∫—Å—ã (–µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç ‚Äî –≤–µ—Ä–Ω—ë–º None)
        def idx(col):
            try:
                return cols.index(col)
            except ValueError:
                return None

        i_secid     = idx("SECID")
        i_shortname = idx("SHORTNAME")
        i_name      = idx("NAME")
        i_secname   = idx("SECNAME")
        i_latname   = idx("LATNAME")
        i_isin      = idx("ISIN")

        created = 0
        for row in data:
            # –¥–æ—Å—Ç–∞—ë–º –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å None
            def get(i):
                return (row[i] if i is not None and i < len(row) else None)

            secid     = get(i_secid)
            shortname = get(i_shortname)
            name      = get(i_name)
            secname   = get(i_secname)
            latname   = get(i_latname)
            isin      = get(i_isin)

            if not secid:
                continue
            ticker = str(secid).strip().upper()

            # 1) —Å–∞–º —Ç–∏–∫–µ—Ä –∫–∞–∫ –∫–ª—é—á
            self.cache[ticker.lower()] = ticker

            # 2) –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ ¬´–∏–º–µ–Ω–∞¬ª –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –∫–ª–∞–¥—ë–º
            for f in (shortname, name, secname, latname, isin):
                if f:
                    key = normalize_name(str(f))
                    if key:
                        self.cache.setdefault(key, ticker)

            # 3) –±–∞–∑–æ–≤–æ–µ —Ä—É—Å—Å–∫–æ–µ –∏–º—è –±–µ–∑ –º–µ—Ç–æ–∫ –∞–æ/–∞–ø –∏ –µ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç—ã
            base_ru = None
            if shortname:
                base_ru = normalize_name(
                    re.sub(r"\b(–∞–ø|–∞–æ|pref|–ø—Ä–µ—Ñ|pr)\b", " ", str(shortname), flags=re.IGNORECASE)
                )
            if not base_ru and name:
                base_ru = normalize_name(str(name))

            def is_pref_from_shortname(sn: Optional[str]) -> bool:
                s = (sn or "").lower()
                return any(x in s for x in [" –∞–ø", " –ø—Ä–µ—Ñ", " pref", " pr"])

            if base_ru:
                if is_pref_from_shortname(shortname):
                    for v in (f"{base_ru} –∞–ø", f"{base_ru} pref", f"{base_ru} –ø—Ä–µ—Ñ"):
                        self.cache.setdefault(v, ticker)
                else:
                    for v in (f"{base_ru} –∞–æ", f"{base_ru} ao"):
                        self.cache.setdefault(v, ticker)

            created += 1

        print(f"–°–æ–∑–¥–∞–Ω–æ {created} –±—É–º–∞–≥, –≤—Å–µ–≥–æ –∫–ª—é—á–µ–π –≤ –∫—ç—à–µ: {len(self.cache)}")


    def _apply_manual_mappings(self):
        for k, v in (self.manual_mappings or {}).items():
            self.cache[normalize_name(k)] = v.upper()
            # –∏ –ø—Ä—è–º–æ–π —Ç–∏–∫–µ—Ä-–∫–ª—é—á
            self.cache[k.lower()] = v.upper()

    def get_ticker(self, company_name: str) -> Optional[str]:
        if not company_name:
            return None
        raw = company_name.strip()
        # 1) –µ—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Ç–∏–∫–µ—Ä (–≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –∏ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤) ‚Äî –≤–µ—Ä–Ω–µ–º –∫–∞–∫ –µ—Å—Ç—å,
        # –Ω–æ –≤—Å–µ —Ä–∞–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏–º –Ω–∞–ª–∏—á–∏–µ –≤ –∫—ç—à–µ
        direct_key = raw.lower()
        if direct_key in self.cache:
            return self.cache[direct_key]

        # 2) –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É
        key = normalize_name(raw)
        if key in self.cache:
            return self.cache[key]

        # 3) –ø—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è ¬´-–∞–æ/-–∞–ø¬ª —á–µ—Ä–µ–∑ –¥–µ—Ñ–∏—Å—ã
        key_dash = normalize_name(raw.replace("-", " "))
        if key_dash in self.cache:
            return self.cache[key_dash]

        return None

    def replace_tickers_in_record(self, record: Dict, context: Optional[str] = None):
            """
            –ó–∞–º–µ–Ω—è–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –Ω–∞ —Ç–∏–∫–µ—Ä—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂:
            (–æ–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è_–∑–∞–ø–∏—Å—å, stats_dict)

            stats_dict = {
                "not_found": set[str],           # –∫–∞–∫–∏–µ –∏–º–µ–Ω–∞ –Ω–µ –Ω–∞—à–ª–∏
                "removed_in_scope": int,         # —Å–∫–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤—ã–∫–∏–Ω—É–ª–∏ –∏–∑ scope.tickers
                "removed_in_sentiment": int,     # —Å–∫–æ–ª—å–∫–æ –∏–∑ sentiment.by_ticker
                "removed_in_target": int,        # —Å–∫–æ–ª—å–∫–æ –∏–∑ target.tickers
                "kept_total": int,               # —Å–∫–æ–ª—å–∫–æ –æ—Å—Ç–∞–≤–∏–ª–∏ —Å—É–º–º–∞—Ä–Ω–æ
            }
            """
            work = record.get("result", record)
            stats = {
                "not_found": set(),
                "removed_in_scope": 0,
                "removed_in_sentiment": 0,
                "removed_in_target": 0,
                "kept_total": 0,
            }

            def _replace_in_list(lst, bucket_name):
                if not isinstance(lst, list):
                    return []
                out = []
                for obj in lst:
                    old = obj.get("ticker")
                    tk = self.get_ticker(old)
                    if tk:
                        new_obj = dict(obj)
                        new_obj["ticker"] = tk
                        out.append(new_obj)
                    else:
                        stats["not_found"].add(str(old))
                        stats[f"removed_in_{bucket_name}"] += 1
                return out

            # –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ¬´–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤¬ª –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞
            # (—á—Ç–æ–±—ã ¬´–Ω–µ –Ω–∞–π–¥–µ–Ω–æ¬ª –≤—ã–≤–æ–¥–∏–ª–æ—Å—å –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ –∏–º—è)
            # ‚Äî –º—ã –∏ —Ç–∞–∫ —Å–æ–±–∏—Ä–∞–µ–º –≤ stats["not_found"].

            if "scope" in work and "tickers" in work["scope"]:
                work["scope"]["tickers"] = _replace_in_list(work["scope"]["tickers"], "scope")

            if "sentiment" in work and "by_ticker" in work["sentiment"]:
                work["sentiment"]["by_ticker"] = _replace_in_list(work["sentiment"]["by_ticker"], "sentiment")

            if "target" in work and "tickers" in work["target"]:
                work["target"]["tickers"] = _replace_in_list(work["target"]["tickers"], "target")

            # –ü–æ–¥—Å—á—ë—Ç –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è
            kept = 0
            kept += len(work.get("scope", {}).get("tickers", []) or [])
            kept += len(work.get("sentiment", {}).get("by_ticker", []) or [])
            kept += len(work.get("target", {}).get("tickers", []) or [])
            stats["kept_total"] = kept

            # –õ–æ–≥–∏ –ø–æ ¬´–Ω–µ –Ω–∞–π–¥–µ–Ω–æ¬ª (—Ä–∞–∑–æ–≤–æ –Ω–∞ –∑–∞–ø–∏—Å—å)
            if self.log_not_found and stats["not_found"]:
                where = f" [{context}]" if context else ""
                for name in sorted(stats["not_found"]):
                    print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Ç–∏–∫–µ—Ä –¥–ª—è: '{name}'{where}")

            return record, stats

    def process_jsonl_file(self, input_file: str, output_file: str):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç JSONL. –ï—Å–ª–∏ drop_records_without_tickers=True,
        –∑–∞–ø–∏—Å–∏ –±–µ–∑ –µ–¥–∏–Ω–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –ù–ï –ø–∏—à–µ–º (–∏ –ª–æ–≥–∏—Ä—É–µ–º, –ø–æ—á–µ–º—É).
        """
        print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {input_file}")
        processed = 0
        written = 0
        dropped = 0

        import ast

        with open(input_file, "r", encoding="utf-8") as fin, open(output_file, "w", encoding="utf-8") as fout:
            for line_num, line in enumerate(fin, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    try:
                        rec = json.loads(line)
                    except json.JSONDecodeError:
                        rec = ast.literal_eval(line)

                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ª–æ–≥–æ–≤
                    rec_id = rec.get("_id") or rec.get("id") or ""
                    title = rec.get("title") or rec.get("result", {}).get("title") or ""
                    ctx = f"—Å—Ç—Ä–æ–∫–∞ {line_num}" + (f", id={rec_id}" if rec_id else "") + (f", '{title[:60]}'" if title else "")

                    rec2, stats = self.replace_tickers_in_record(rec, context=ctx)

                    if self.drop_records_without_tickers and stats["kept_total"] == 0:
                        # –Ω–∏—á–µ–≥–æ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å ‚Äî –¥—Ä–æ–ø–∞–µ–º –∏ —è–≤–Ω–æ –ª–æ–≥–∏—Ä—É–µ–º
                        print(f"üóë  –ó–∞–ø–∏—Å—å —É–¥–∞–ª–µ–Ω–∞: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤ ({ctx}). "
                              f"–£–¥–∞–ª–µ–Ω–æ: scope={stats['removed_in_scope']}, "
                              f"sentiment={stats['removed_in_sentiment']}, target={stats['removed_in_target']}")
                        dropped += 1
                    else:
                        fout.write(json.dumps(rec2, ensure_ascii=False) + "\n")
                        written += 1

                    processed += 1
                    if processed % 500 == 0:
                        print(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}, –∑–∞–ø–∏—Å–∞–Ω–æ {written}, —É–¥–∞–ª–µ–Ω–æ {dropped}")

                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {line_num}: {e}")
                    continue

        print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}, –∑–∞–ø–∏—Å–∞–Ω–æ: {written}, —É–¥–∞–ª–µ–Ω–æ: {dropped}")
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {output_file}")

manual_mappings = {
    '–±–∞–Ω–∫ –†–æ—Å—Å–∏—è': 'RTKM',
    '–í–¢–ë –ö–∞–ø–∏—Ç–∞–ª': 'VTBR',
    '–†–ù-–Æ–≥–∞–Ω—Å–∫–Ω–µ—Ñ—Ç–µ–≥–∞–∑ ': 'ROSN',
    '–ù–æ–≤–æ–∫—É–π–±—ã—à–µ–≤—Å–∫–∏–π –∑–∞–≤–æ–¥ –∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤': 'ROSN',
    'Yandex N.V.': 'YDEX',
    '–ì–∞–∑–ø—Ä–æ–º —Ç—Ä–∞–Ω—Å–≥–∞–∑ –¢–æ–º—Å–∫': 'GAZP',
    '–ì–∞–∑–ø—Ä–æ–º –≥–∞–∑–æ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ': 'GAZP',
    '–ì–∞–∑–ø—Ä–æ–º –¥–æ–±—ã—á–∞ –Ø–º–±—É—Ä–≥': 'GAZP',
    '–ì–∞–∑–ø—Ä–æ–º —ç–∫—Å–ø–æ—Ä—Ç': 'GAZP',
    '–ì–∞–∑–ø—Ä–æ–º —Ç—Ä–µ–π–¥–∏–Ω–≥': 'GAZP',
    '–ì–∞–∑–ø—Ä–æ–º –¥–æ–±—ã—á–∞ –û—Ä–µ–Ω–±—É—Ä–≥': 'GAZP',
    'RUSAL Plc': 'RUAL',
    'Tele2': 'RTKM',
    '–¢–µ–ª–µ2': 'RTKM ',
    '–¢2 –ú–æ–±–∞–π–ª': 'RTKM',
    '–ì–∞–∑–ø—Ä–æ–º –ú–µ–∂—Ä–µ–≥–∏–æ–Ω–≥–∞–∑': 'GAZP',
    '–°–µ–≤–µ—Ä—Å—Ç–∞–ª—å-–º–µ—Ç–∏–∑': 'CHMF',
    '–ê–ö –ê–õ–†–û–°–ê': 'ALRS',
    '–Ø–Ω–¥–µ–∫—Å.–ï–¥–∞': 'YDEX',
    '–õ–µ–Ω—Ç–∞ –¥—Ä': 'LNTA',
    '–•5': 'X5',
    '–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –ö—Ä–µ–¥–∏—Ç–Ω—ã–π –ë–∞–Ω–∫': 'CBOM',
    'Delivery Club': 'SBER',
    '–†–æ—Å—Å–∏–π—Å–∫–∏–µ —Å–µ—Ç–∏': 'FEES',
    'Rosneft Trading': 'ROSN',
    '–ë–∞—à–Ω–µ—Ñ—Ç—å': 'BANEP',
    '–°–ê–§–ú–ê–† –§–ò': 'SFIN',
    'GAZP': 'GAZP',
    'AFLT': 'AFLT',
    'MVID': 'MVID',
    'SBER': 'SBER',
    'LKOH': 'LKOH',
    'DSKY': 'DSKY',
    'SFIN': 'SFIN',
    '–ú–ì–ù–¢ ': 'MGNT',
    'FIVE': 'X5',
    'NLMK': 'NLMK',
    'MOEX': 'MOEX',
    'MAGN': 'MAGN',
    'PLZL': 'PLZL',
    'ALRS': 'ALRS',
    'PHOR': 'PHOR',
    'IRAO': 'IRAO',
    'POLY': 'POLY',
    'CHMF': 'CHMF',
    'PLZL': 'PLZL',
    'GAZP': 'GAZP',
    '–ú–µ—á–µ–ª-–∞–æ': 'GAZP',
    '–°—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑-–∞–æ': 'SNGS',
    '–°—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑': 'SNGS',
    '–†–æ—Å—Å–µ—Ç–∏-–∞–ø': 'MSRS',
    '–ë–∞–Ω–∫ –í–¢–ë': 'VTBR',
    '–¢–∞—Ç–Ω–µ—Ñ—Ç—å-–∞–ø': 'TATN',
    '–ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å': 'GAZP',
    '–ü—Ä–æ–º—Å–≤—è–∑—å–±–∞–Ω–∫': 'PSBR',
    '–ú–æ—Å—ç–Ω–µ—Ä–≥–æ': 'MSNG',
    'X5': 'X5',
    '–†–æ—Å—Å–µ—Ç–∏-–∞–æ': 'MSRS',
    '–¢–∞—Ç–Ω–µ—Ñ—Ç—å-–∞–æ': 'TATN',
    '–Ø–Ω–¥–µ–∫—Å.–¢–∞–∫—Å–∏': 'YDEX',
    '–ê–ø—Ç–µ–∫–∏ 36,6': 'APTK',
    '–ê–ø—Ç–µ–∫–∏ 36–∏6': 'APTK',
    '–Æ–¢—ç–π—Ä': 'UTAR',
    '–ü—è—Ç–µ—Ä–æ—á–∫–∞': 'X5',
    '–£—Ä–∞–ª–∫–∞–ª–∏–π': 'URKA',
    '–ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å-–ê—ç—Ä–æ': 'GAZP',
    '–ù–æ–≤–∞—Ç–µ–∫': 'NVTK',
    '–ú–ö–ë': 'CBOM',
    'Sberbank CIB': 'SBER',
    '–¢–∏–Ω—å–∫–æ—Ñ—Ñ –ë–∞–Ω–∫': '–¢',
    '–ë–∞–Ω–∫ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥': 'BSPB',
    '–ö—É–±–∞–Ω—å—ç–Ω–µ—Ä–≥–æ': 'KBSB',
    '–õ–°–†': 'LSRG',
    'Mail.Ru Group': 'MAIL',
    'Mail.ru': 'MAIL',
    '–°–∏—Ç–∏–º–æ–±–∏–ª': 'VKCO',
    'Nord Steam 2': 'GAZP',
    'Nord Stream 2': 'GAZP',
    'Nord Stream': 'GAZP',
    '–ú–†–°–ö –¶–ü': 'MRKP',
    '–°–ê–§–ú–ê–†': 'SFIN',
    '–ê–§–ö –°–∏—Å—Ç–µ–º–∞': 'AFKS',
    '–ù–æ—Ä–∏–ª—å—Å–∫–∏–π –ù–∏–∫–µ–ª—å': 'GMKN',
    '–¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å': 'TRNFP',
    '–õ–µ–Ω—ç–Ω–µ—Ä–≥–æ': 'LSNGP',
    '–ú–µ—á–µ–ª': 'MTLR',
    '–≥–∞–∑–ø—Ä–æ–º': 'GAZP',
    '–ª—É–∫–æ–π–ª': 'LKOH',
    '—Å–±–µ—Ä–±–∞–Ω–∫': 'SBER',
    '—è–Ω–¥–µ–∫—Å': 'YDEX',
    '—Ä–æ—Å–Ω–µ—Ñ—Ç—å': 'ROSN',
    '–Ω–æ–≤–∞—Ç—ç–∫': 'NVTK',
    '—Ç–∞—Ç–Ω–µ—Ñ—Ç—å': 'TATN',
    '–º–∞–≥–Ω–∏—Ç': 'MGNT',
    '–Ω–æ—Ä–Ω–∏–∫–µ–ª—å': 'GMKN',
    '–ø–æ–ª—é—Å': 'PLZL',
    'vtb': 'VTBR',
    '–≤—Ç–±': 'VTBR',
    '–º—Ç—Å': 'MTSS',
    '–∞–ª—Ä–æ—Å–∞': 'ALRS',
    '—Å—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑': 'SNGS',
    '—Å–µ–≤–µ—Ä—Å—Ç–∞–ª—å': 'CHMF',
    'polymetal': 'POLY',
    '–ø–æ–ª–∏–º–µ—Ç–∞–ª–ª': 'POLY',
    '–∞—ç—Ä–æ—Ñ–ª–æ—Ç': 'AFLT',
    '–º–æ—Å–±–∏—Ä–∂–∞': 'MOEX',
    '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ': 'TCSG',
    'ozon': 'OZON',
    '–æ–∑–æ–Ω': 'OZON',
    '—Ñ–∏–∫—Å –ø—Ä–∞–π—Å': 'FIXP',
    '—Ñ–æ—Å–∞–≥—Ä–æ': 'PHOR',
    '—Ä—É—Å–∞–ª': 'RUAL',
    '–Ω–ª–º–∫': 'NLMK',
    '–ø–∏–∫': 'PIKK',
    '–∏–Ω—Ç–µ—Ä —Ä–∞–æ': 'IRAO',
    'X5 Retail Group': 'X5',
    '–ß–¢–ü–ó': 'CHEP',
    '–ì–ú–ö–ù': 'GMKN',
    '–¢–ú–ö': 'TRMK',
    '–ù–ú–¢–ü': 'NMTP',
    '–ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å': 'SIBN',
    '–ú–ú–ö': 'MAGN',
    '–ú–∞–≥–Ω–∏—Ç–æ–≥–æ—Ä—Å–∫–∏–π –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏–π –∫–æ–º–±–∏–Ω–∞—Ç': 'MAGN',
    '–î–µ—Ç—Å–∫–∏–π –ú–∏—Ä': 'DSKY',
    'SberCloud': 'SBER',
    '–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞': 'MOEX',
    '–Ø–Ω–¥–µ–∫—Å.–î—Ä–∞–π–≤': 'YDEX',
    '–†–æ—Å—Ç–µ–ª–µ–∫–æ–º': 'RTKM',
    '–ò–Ω—Ç–µ—Ä–†–∞–æ ': 'IRAO',
    'FIVE-–≥–¥—Ä': 'X5',
    'Yandex': 'YDEX',
    '–ì–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫': 'GAZP',
    '–¢—Ä–∞–Ω—Å–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä': 'TRCN ',
    '–¢—Ä–∞–Ω—Å–ö': 'TRCN',
    '–£—Ä–∞–ª—å—Å–∫–∏–µ –∞–≤–∏–∞–ª–∏–Ω–∏–∏': 'URAL',
    '–ü–æ–±–µ–¥–∞': 'AFLT',
    '–†–æ—Å—Å–∏—è': 'AFLT',
    '–Ø–Ω–¥–µ–∫—Å.–î–µ–Ω—å–≥–∏': 'YDEX',
}

def unite_all_batches(root_dir_name: str = "artifacts", output_file_name: str = "merged_parsed.jsonl"):
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã parsed.jsonl –∏–∑ –ø–∞–ø–æ–∫ batch_00000, batch_00001 –∏ —Ç.–¥.
    –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª merged_parsed.jsonl.
    """
    root_dir = Path(root_dir_name)
    output_file = root_dir / output_file_name
    with open(output_file, "w", encoding="utf-8") as outfile:
        for batch_dir in sorted(root_dir.iterdir()):
            parsed_path = batch_dir / "parsed.jsonl"
            if parsed_path.exists():
                with open(parsed_path, "r", encoding="utf-8") as infile:
                    for line in infile:
                        outfile.write(line)
    print(f"–ì–æ—Ç–æ–≤–æ! –í—Å–µ parsed.jsonl –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –≤ {output_file}")
    
# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–∞—Ç—á–∏ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
    unite_all_batches(root_dir_name="artifacts", output_file_name="merged_parsed.jsonl")
    
    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–µ—Ä
    mapper = MOEXTickerMapper(manual_mappings=manual_mappings)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
    mapper.process_jsonl_file(
        input_file=r'merged_parsed.jsonl',
        output_file='llm_news_file.jsonl'
    )
    
    # –ü—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤:
    print("\n--- –ü—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–∞ ---")
    test_names = ['–ì–∞–∑–ø—Ä–æ–º', '–õ–£–ö–û–ô–õ', '–°–±–µ—Ä–±–∞–Ω–∫', '–Ø–Ω–¥–µ–∫—Å']
    for name in test_names:
        ticker = mapper.get_ticker(name)
        print(f"{name} -> {ticker}")